{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from typing import Dict, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175461</th>\n",
       "      <td>We need to uphold laws against discrimination ...</td>\n",
       "      <td>Nous devons faire respecter les lois contre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175462</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175463</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175464</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175465</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "0                                                     Hi.   \n",
       "1                                                    Run!   \n",
       "2                                                    Run!   \n",
       "3                                                    Who?   \n",
       "4                                                    Wow!   \n",
       "...                                                   ...   \n",
       "175461  We need to uphold laws against discrimination ...   \n",
       "175462  A carbon footprint is the amount of carbon dio...   \n",
       "175463  Death is something that we're often discourage...   \n",
       "175464  Since there are usually multiple websites on a...   \n",
       "175465  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                   output  \n",
       "0                                                  Salut!  \n",
       "1                                                 Cours !  \n",
       "2                                                Courez !  \n",
       "3                                                   Qui ?  \n",
       "4                                              Ça alors !  \n",
       "...                                                   ...  \n",
       "175461  Nous devons faire respecter les lois contre la...  \n",
       "175462  Une empreinte carbone est la somme de pollutio...  \n",
       "175463  La mort est une chose qu'on nous décourage sou...  \n",
       "175464  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "175465  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "\n",
       "[175466 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"en-2-fr-translation.parquet\", engine='pyarrow').rename(columns={'English words/sentences': 'input', 'French words/sentences': 'output'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jsonl files\n",
    "df.to_json('en-2-fr-translation.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 342.92it/s]\n",
      "Generating train split: 175466 examples [00:00, 3527675.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 175466\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play around with dataset / tokenizer\n",
    "train_dataset = datasets.load_dataset('json', data_files='en-2-fr-translation.jsonl')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from dataset import fmt_prompt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.34k/7.34k [00:00<00:00, 3.83MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 6.83MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.16MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 489kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 42.8kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 14.7MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        'microsoft/phi-2',\n",
    "        model_max_length=2048,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        # pad_token=DEFAULT_PAD_TOKEN,\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        samples: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer\n",
    "    ) -> Dict:\n",
    "    \"\"\"Preprocess data for training by tokenizing\"\"\"\n",
    "    sources = [f\"{fmt_prompt(sources)}\" for sources in samples[\"input\"]]\n",
    "    targets = [f\"{translation}{tokenizer.eos_token}\" for translation in samples[\"output\"]]\n",
    "    complete_examples = [s + t for s,t in zip(sources, targets)]\n",
    "    \"\"\"tokenize examples\"\"\"\n",
    "    tokenized_strings = [\n",
    "        tokenizer(\n",
    "            example,\n",
    "            return_tensors='pt',\n",
    "            padding=False,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) \n",
    "        for example in complete_examples\n",
    "    ]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    \"\"\"Dataset for fine-tuning model\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, paths: str, limit=3000):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        dataset = (\n",
    "            datasets.load_dataset(\n",
    "            \"json\",\n",
    "            data_files=paths,\n",
    "            split=f\"train[0:{limit}]\" if limit else \"train\",\n",
    "            )\n",
    "            # .filter(\n",
    "            #     # filter data entries\n",
    "            #     )\n",
    "            .map(\n",
    "                lambda samples: preprocess(samples, tokenizer),\n",
    "                batched=True,\n",
    "                batch_size=300,\n",
    "                # create a preprocessing function \n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = None \n",
    "        # self.size = len(dataframe)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 6220.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataSet(tokenizer, ['en-2-fr-translation.jsonl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
