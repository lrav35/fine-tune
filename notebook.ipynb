{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from typing import Dict, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175461</th>\n",
       "      <td>We need to uphold laws against discrimination ...</td>\n",
       "      <td>Nous devons faire respecter les lois contre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175462</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175463</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175464</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175465</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "0                                                     Hi.   \n",
       "1                                                    Run!   \n",
       "2                                                    Run!   \n",
       "3                                                    Who?   \n",
       "4                                                    Wow!   \n",
       "...                                                   ...   \n",
       "175461  We need to uphold laws against discrimination ...   \n",
       "175462  A carbon footprint is the amount of carbon dio...   \n",
       "175463  Death is something that we're often discourage...   \n",
       "175464  Since there are usually multiple websites on a...   \n",
       "175465  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                   output  \n",
       "0                                                  Salut!  \n",
       "1                                                 Cours !  \n",
       "2                                                Courez !  \n",
       "3                                                   Qui ?  \n",
       "4                                              Ça alors !  \n",
       "...                                                   ...  \n",
       "175461  Nous devons faire respecter les lois contre la...  \n",
       "175462  Une empreinte carbone est la somme de pollutio...  \n",
       "175463  La mort est une chose qu'on nous décourage sou...  \n",
       "175464  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "175465  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "\n",
       "[175466 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"en-2-fr-translation.parquet\", engine='pyarrow').rename(columns={'English words/sentences': 'input', 'French words/sentences': 'output'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create jsonl files\n",
    "df.to_json('en-2-fr-translation.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 342.92it/s]\n",
      "Generating train split: 175466 examples [00:00, 3527675.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 175466\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play around with dataset / tokenizer\n",
    "train_dataset = datasets.load_dataset('json', data_files='en-2-fr-translation.jsonl')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from dataset import fmt_prompt\n",
    "import os\n",
    "import copy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        'microsoft/phi-2',\n",
    "        model_max_length=2048,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        pad_token=\"<|pad|>\",\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(\n",
    "        strings: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer\n",
    ") -> Dict:\n",
    "    \"\"\"tokenize examples\"\"\"\n",
    "    tokenized_strings = [\n",
    "        tokenizer(\n",
    "            example,\n",
    "            return_tensors='pt',\n",
    "            padding=False,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) \n",
    "        for example in strings\n",
    "    ]\n",
    "\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_strings]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_strings\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "        samples: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer\n",
    "    ) -> Dict:\n",
    "    \"\"\"Preprocess data for training by tokenizing\"\"\"\n",
    "    sources = [f\"{fmt_prompt(sources)}\" for sources in samples[\"input\"]]\n",
    "    targets = [f\"{translation}{tokenizer.eos_token}\" for translation in samples[\"output\"]]\n",
    "    complete_examples = [s + t for s,t in zip(sources, targets)] # source + target -> \"Can you translate this phrase for me? <|phrase|>, Sure thing, here is the french translation <|target|>\"\n",
    "    examples_tokenized, sources_tokenized = [\n",
    "        _tokenize(strings, tokenizer) for strings in (complete_examples, sources)\n",
    "    ]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_length in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_length] = -100 # Pytorch will ignore -100 during learning in c.e.l.\n",
    "    return dict(input_ids=input_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    \"\"\"Dataset for fine-tuning model\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, paths: str, limit=3000):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        dataset = (\n",
    "            datasets.load_dataset(\n",
    "            \"json\",\n",
    "            data_files=paths,\n",
    "            split=f\"train[0:{limit}]\" if limit else \"train\",\n",
    "            )\n",
    "            .map(\n",
    "                lambda samples: preprocess(samples, tokenizer),\n",
    "                batched=True,\n",
    "                batch_size=300,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = dataset[\"input_ids\"]\n",
    "        self.labels = dataset[\"labels\"]\n",
    "        # self.size = len(dataframe)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids = torch.tensor(self.input_ids[idx]),\n",
    "            labels = torch.tensor(self.labels[idx])\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataSet(tokenizer, ['en-2-fr-translation.jsonl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([21017, 27759,    25,   198,  1680,   345,  3387, 15772,   428,  9546,\n",
       "           393,  1573,   284, 48718,    30,   220,   198,   775,   821, 15800,\n",
       "            13,   198,   198, 21017, 18261,    25,   198,  3363,   286,  1781,\n",
       "             0,  3423,   318,   257, 48718, 11059,   286,   326,  9546,    25,\n",
       "           220,   198,    45,   516,   264,  2002,   274,  4628,  1460,    13,\n",
       "         50256]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,    45,   516,   264,  2002,   274,  4628,  1460,    13,\n",
       "         50256])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### wandb experimenting ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlrav35\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/n0342839/code/personal/fine-tune/wandb/run-20240111_094742-wcr8eavz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lrav35/my-awesome-project/runs/wcr8eavz' target=\"_blank\">snowy-snowflake-1</a></strong> to <a href='https://wandb.ai/lrav35/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lrav35/my-awesome-project' target=\"_blank\">https://wandb.ai/lrav35/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lrav35/my-awesome-project/runs/wcr8eavz' target=\"_blank\">https://wandb.ai/lrav35/my-awesome-project/runs/wcr8eavz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▂█▅█▇█▇</td></tr><tr><td>loss</td><td>█▆▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.90225</td></tr><tr><td>loss</td><td>0.11831</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-snowflake-1</strong> at: <a href='https://wandb.ai/lrav35/my-awesome-project/runs/wcr8eavz' target=\"_blank\">https://wandb.ai/lrav35/my-awesome-project/runs/wcr8eavz</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240111_094742-wcr8eavz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
